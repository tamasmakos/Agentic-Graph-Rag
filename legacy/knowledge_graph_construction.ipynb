{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (5.25.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: kor in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: langchain_groq in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: langchain_core in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: langchain_experimental in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: yfiles_jupyter_graphs in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: pyvis in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: graphdatascience in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (1.12)\n",
      "Requirement already satisfied: langgraph in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (0.2.39)\n",
      "Requirement already satisfied: pytz in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from neo4j) (2024.2)\n",
      "Requirement already satisfied: pandas<3,>=1.5.3 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from kor) (2.2.3)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from kor) (2.9.2)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain_core) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain_core) (0.1.132)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain_core) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain_core) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain_core) (4.12.2)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain_experimental) (0.3.2)\n",
      "Requirement already satisfied: ipywidgets>=8.0.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from yfiles_jupyter_graphs) (8.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from pyvis) (8.28.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from pyvis) (3.1.4)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from sentence_transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from sentence_transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from sentence_transformers) (0.25.2)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from graphdatascience) (1.12)\n",
      "Requirement already satisfied: pyarrow<17.0,>=14.0.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from graphdatascience) (16.1.0)\n",
      "Requirement already satisfied: textdistance<5.0,>=4.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from graphdatascience) (4.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from graphdatascience) (2.32.3)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langgraph) (2.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.32 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langgraph) (0.1.33)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.9.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipywidgets>=8.0.0->yfiles_jupyter_graphs) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipywidgets>=8.0.0->yfiles_jupyter_graphs) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from ipywidgets>=8.0.0->yfiles_jupyter_graphs) (3.0.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (3.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10.9)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.3 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.3)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.5.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph) (1.1.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.4.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from pandas<3,>=1.5.3->kor) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from pydantic<3,>=2->kor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from pydantic<3,>=2->kor) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from requests->graphdatascience) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from requests->graphdatascience) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from requests->graphdatascience) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from requests->graphdatascience) (2024.8.30)\n",
      "Requirement already satisfied: sympy in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.20.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.13.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from langchain<0.4.0,>=0.3.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.1.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\makos2tamas911\\documents\\dev\\mm\\graphrag_langgraph_neo4j\\virtual\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install neo4j kor langchain_groq langchain_core langchain_experimental yfiles_jupyter_graphs networkx matplotlib pyvis sentence_transformers graphdatascience langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.5,\n",
    "    max_bucket_size=500,\n",
    ")\n",
    "\n",
    "# Define the language model (LLM) once\n",
    "llm = ChatGroq(\n",
    "    temperature=0.0,\n",
    "    model_name=\"llama-3.2-11b-text-preview\",\n",
    "    rate_limiter=rate_limiter,\n",
    "    api_key=\"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\"\n",
    ")\n",
    "\n",
    "# Initialize the LLMGraphTransformer\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    strict_mode=True,\n",
    "    node_properties=True,\n",
    "    relationship_properties=True,\n",
    "    ignore_tool_usage=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during knowledge graph generation: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': ' {\"nodes\": [{\"id\": \"Faust\", \"type\": \"person\"}, {\"id\": \"Spring\", \"type\": \"season\"}, {\"id\": \"Hope\", \"type\": \"emotion\"}, {\"id\": \"Winter\", \"type\": \"season\"}, {\"id\": \"sun\", \"type\": \"celestial body\"}, {\"id\": \"city\", \"type\": \"location\"}, {\"id\": \"gate\", \"type\": \"structure\"}, {\"id\": \"Lord\", \"type\": \"deity\"}, {\"id\": \"resurrection\", \"type\": \"event\"}, {\"id\": \"dwellings\", \"type\": \"structure\"}, {\"id\": \"labour\", \"type\": \"activity\"}, {\"id\": \"business\", \"type\": \"activity\"}, {\"id\": \"churches\", \"type\": \"structure\"}, {\"id\": \"multitude\", \"type\": \"group\"}, {\"id\": \"fields\", \"type\": \"location\"}, {\"id\": \"gardens\", \"type\": \"location\"}, {\"id\": \"wherry\", \"type\": \"vehicle\"}, {\"id\": \"hills\", \"type\": \"location\"}, {\"id\": \"hamlet\", \"type\": \"location\"}, {\"id\": \"Wagner\", \"type\": \"person\"}], \"relationships\": [{\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Spring\", \"target_node_type\": \"season\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Hope\", \"target_node_type\": \"emotion\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Winter\", \"target_node_type\": \"season\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"sun\", \"target_node_type\": \"celestial body\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"city\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"gate\", \"target_node_type\": \"structure\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Lord\", \"target_node_type\": \"deity\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"resurrection\", \"target_node_type\": \"event\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"dwellings\", \"target_node_type\": \"structure\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"labour\", \"target_node_type\": \"activity\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"business\", \"target_node_type\": \"activity\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"churches\", \"target_node_type\": \"structure\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"multitude\", \"target_node_type\": \"group\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"fields\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"gardens\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"wherry\", \"target_node_type\": \"vehicle\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"hills\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"hamlet\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Wagner\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"acquaintance\"}]}'}}\n",
      "Error during classification: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-11b-text-preview` in organization `org_01j93px8kqec5sssgk0j7h2t9a` on tokens per minute (TPM): Limit 7000, Used 6887, Requested 3110. Please try again in 25.682142857s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Error during knowledge graph generation: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': ' {\"nodes\": [{\"id\": \"Margaret\", \"type\": \"person\"}, {\"id\": \"Faust\", \"type\": \"person\"}, {\"id\": \"Henry\", \"type\": \"person\"}, {\"id\": \"parson\", \"type\": \"person\"}, {\"id\": \"God\", \"type\": \"entity\"}], \"relationships\": [{\"source_node_id\": \"Margaret\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Margaret\", \"target_node_type\": \"person\", \"type\": \"LOVE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"God\", \"target_node_type\": \"entity\", \"type\": \"BELIEF\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"parson\", \"target_node_type\": \"person\", \"type\": \"INFLUENCE\"}, {\"source_node_id\": \"Margaret\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"CONCERN\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Margaret\", \"target_node_type\": \"person\", \"type\": \"CONCERN\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"FRIEND\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"LOVER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"PARTNER\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"SPOUSE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"MATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"COMPANION\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ACQUAINTANCE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"ASSOCIATE\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id'}}\n",
      "Error during knowledge graph generation for chunk: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': ' {\"nodes\": [{\"id\": \"Faust\", \"type\": \"person\"}, {\"id\": \"Spring\", \"type\": \"season\"}, {\"id\": \"Hope\", \"type\": \"emotion\"}, {\"id\": \"Winter\", \"type\": \"season\"}, {\"id\": \"sun\", \"type\": \"celestial body\"}, {\"id\": \"city\", \"type\": \"location\"}, {\"id\": \"gate\", \"type\": \"structure\"}, {\"id\": \"Lord\", \"type\": \"deity\"}, {\"id\": \"resurrection\", \"type\": \"event\"}, {\"id\": \"dwellings\", \"type\": \"structure\"}, {\"id\": \"labour\", \"type\": \"activity\"}, {\"id\": \"business\", \"type\": \"activity\"}, {\"id\": \"churches\", \"type\": \"structure\"}, {\"id\": \"multitude\", \"type\": \"group\"}, {\"id\": \"fields\", \"type\": \"location\"}, {\"id\": \"gardens\", \"type\": \"location\"}, {\"id\": \"wherry\", \"type\": \"vehicle\"}, {\"id\": \"hills\", \"type\": \"location\"}, {\"id\": \"hamlet\", \"type\": \"location\"}, {\"id\": \"Wagner\", \"type\": \"person\"}, {\"id\": \"Doctor\", \"type\": \"title\"}], \"relationships\": [{\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Spring\", \"target_node_type\": \"season\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Hope\", \"target_node_type\": \"emotion\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Winter\", \"target_node_type\": \"season\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"sun\", \"target_node_type\": \"celestial body\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"city\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"gate\", \"target_node_type\": \"structure\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"Lord\", \"target_node_type\": \"deity\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"resurrection\", \"target_node_type\": \"event\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"dwellings\", \"target_node_type\": \"structure\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"labour\", \"target_node_type\": \"activity\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"business\", \"target_node_type\": \"activity\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"churches\", \"target_node_type\": \"structure\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"multitude\", \"target_node_type\": \"group\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"fields\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"gardens\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"wherry\", \"target_node_type\": \"vehicle\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"hills\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Faust\", \"source_node_type\": \"person\", \"target_node_id\": \"hamlet\", \"target_node_type\": \"location\", \"type\": \"associated_with\"}, {\"source_node_id\": \"Wagner\", \"source_node_type\": \"person\", \"target_node_id\": \"Faust\", \"target_node_type\": \"person\", \"type\": \"acquaintance\"}, {\"source_node_id\": \"Wagner\", \"source_node_type\": \"person\", \"target_node_id\": \"Doctor\", \"target_node_type\": \"title\", \"type\": \"associated_with\"}]}'}}\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n",
      "Error during knowledge graph generation for chunk: Connection error.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "from kor import create_extraction_chain, Object, Text\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Define the dramatic structure schema using KOR\n",
    "dramatic_structure_schema = Object(\n",
    "    id=\"scene_classification\",\n",
    "    description=\"Classify a scene from Faust according to its dramatic structure\",\n",
    "    attributes=[\n",
    "        Text(\n",
    "            id=\"dramatic_element\",\n",
    "            description=\"The dramatic element that best describes this scene\",\n",
    "            options=[\"Exposition\", \"Rising Action\", \"Climax\", \"Falling Action\", \"Resolution\", \"Structural Elements in Faust\"]\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"justification\",\n",
    "            description=\"A brief explanation of why this scene fits the chosen dramatic element\"\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"scene_description\",\n",
    "            description=\"A brief description of what is happening in the scene based on the eventstream\"\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"scene_title\",\n",
    "            description=\"A suitable title for the scene based on the eventstream\"\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"scene_type\",\n",
    "            description=\"The type of scene based on the eventstream\"\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"scene_knowledge_graph\",\n",
    "            description=\"A knowledge graph representation of the scene based on the eventstream\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the extraction chain\n",
    "classification_chain = create_extraction_chain(llm, dramatic_structure_schema)\n",
    "\n",
    "# Define the dramatic structure dictionary\n",
    "dramatic_structure_dict = {\n",
    "        \"Prologue\": [\n",
    "        \"Dedication: Goethe's reflections on writing the play\",\n",
    "        \"Prelude on the Stage: Discussion between Director, Poet, and Comedian about the nature of drama\",\n",
    "        \"Prologue in Heaven: God's wager with Mephistopheles regarding Faust's soul\"\n",
    "        ],\n",
    "        \"Exposition\": [\n",
    "        \"Introduction of Faust as a dissatisfied scholar in his study\",\n",
    "        \"Faust's attempted suicide and the Easter chorus that saves him\",\n",
    "        \"Faust's walk with Wagner and encounter with the black poodle\",\n",
    "        \"Faust's first encounter with Mephistopheles in his study\"\n",
    "        ],\n",
    "        \"Rising Action\": [\n",
    "        \"Faust's pact with Mephistopheles, signed in blood\",\n",
    "        \"Faust's rejuvenation in the witch's kitchen\",\n",
    "        \"Introduction of Gretchen (Margarete) and the beginning of their romance\",\n",
    "        \"Faust and Mephistopheles' adventures in Auerbach's Cellar\",\n",
    "        \"The gift of jewels to Gretchen and her growing attraction to Faust\",\n",
    "        \"Faust's seduction of Gretchen, aided by Mephistopheles and Martha\"\n",
    "        ],\n",
    "        \"Climax\": [\n",
    "        \"The death of Gretchen's mother due to the sleeping potion\",\n",
    "        \"The duel where Faust kills Gretchen's brother, Valentin\",\n",
    "        \"Gretchen's pregnancy and social ostracism\",\n",
    "        \"The Walpurgis Night scene, contrasting with Gretchen's plight\",\n",
    "        \"Faust's realization of Gretchen's imprisonment\"\n",
    "        ],\n",
    "        \"Falling Action\": [\n",
    "        \"Faust's guilt and desperate attempts to save Gretchen\",\n",
    "        \"The prison scene where Faust tries to convince Gretchen to escape\"\n",
    "        ],\n",
    "        \"Resolution\": [\n",
    "        \"Gretchen's refusal to escape and her execution\",\n",
    "        \"Gretchen's ascension to heaven and redemption\",\n",
    "        \"Mephistopheles' declaration that Faust is still bound to him\"\n",
    "        ],\n",
    "        \"Structural Elements in Faust\": [\n",
    "        \"Framing Device: The play begins and ends with heavenly scenes, emphasizing the cosmic nature of the struggle\",\n",
    "        \"Episodic Structure: The play is composed of loosely connected scenes, allowing for a wide range of experiences and locations\",\n",
    "        \"Parallel Plots: The cosmic struggle between good and evil is mirrored in Faust's personal journey and the tragedy of Gretchen\",\n",
    "        \"Use of Verse and Prose: Goethe alternates between poetic verse and prose to differentiate between elevated and mundane scenes\",\n",
    "        \"Symbolic Characters: Many characters represent broader concepts (e.g., Mephistopheles as temptation, Gretchen as innocence)\",\n",
    "        \"Intertextuality: References to classical mythology, biblical stories, and folk tales enrich the narrative\",\n",
    "        \"Contrast: Juxtaposition of the sublime and the grotesque, the spiritual and the earthly\"\n",
    "        ],\n",
    "        \"Themes Developed Through Structure\": [\n",
    "        \"The limits of human knowledge and ambition\",\n",
    "        \"The conflict between good and evil within the human soul\",\n",
    "        \"The consequences of unchecked desire and ambition\",\n",
    "        \"Redemption through love and divine grace\",\n",
    "        \"The tension between medieval values and Enlightenment ideals\"\n",
    "        ]\n",
    "}\n",
    "\n",
    "def roman_to_int(roman):\n",
    "    roman_dict = {'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5, 'VI': 6, 'VII': 7, 'VIII': 8, 'IX': 9, 'X': 10,\n",
    "                  'XI': 11, 'XII': 12, 'XIII': 13, 'XIV': 14, 'XV': 15, 'XVI': 16, 'XVII': 17, 'XVIII': 18, 'XIX': 19,\n",
    "                  'XX': 20}\n",
    "    return roman_dict.get(roman.upper(), 0)\n",
    "\n",
    "def roman_to_decimal(roman):\n",
    "    # Convert a Roman numeral to a decimal for scene numbering\n",
    "    int_value = roman_to_int(roman)\n",
    "    return int_value\n",
    "\n",
    "def incremental_classify_scene(scene, dramatic_structure_dict, previous_scenes):\n",
    "    # Combine all events in the scene into a single text\n",
    "    scene_text = \"\\n\".join([json.dumps(event) for event in scene['events']])\n",
    "\n",
    "    # Prepare the context from previous scenes\n",
    "    if previous_scenes and len(previous_scenes) > 0:\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Scene {s.get('scene_node_id', 'Unknown')}: {s.get('dramatic_element', 'Unknown')}\"\n",
    "            for s in previous_scenes[-3:] if s is not None\n",
    "        ])\n",
    "    else:\n",
    "        context = \"No previous scenes\"\n",
    "\n",
    "    # Prepare the list of dramatic elements\n",
    "    dramatic_elements_list = \"\\n\".join([\"- \" + key for key in dramatic_structure_dict.keys()])\n",
    "\n",
    "    # Prepare the input for the classification chain\n",
    "    input_text = f\"\"\"\n",
    "                    Previous context:\n",
    "                    {context}\n",
    "\n",
    "                    Classify the following scene from Faust according to its dramatic structure:\n",
    "\n",
    "                    Scene: {scene['scene_node_id']}\n",
    "                    Events:\n",
    "                    {scene_text}\n",
    "\n",
    "                    Dramatic Structure Elements:\n",
    "                    {dramatic_elements_list}\n",
    "\n",
    "                    Classify this scene into one of the dramatic structure elements and provide a detailed justification.\n",
    "                    Consider the context of previous scenes when making your classification.\n",
    "                    Provide the following:\n",
    "                    - Dramatic Element\n",
    "                    - Justification\n",
    "                    - A brief description of what is happening in the scene based on the eventstream\n",
    "                    - A suitable title for the scene based on the eventstream\n",
    "                    - The type of scene based on the eventstream (e.g., dialogue, monologue, action)\n",
    "                    Always provide the classification and additional information, even if you're not entirely certain.\n",
    "                    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Use invoke to get the output\n",
    "        output = classification_chain.invoke({\"text\": input_text})\n",
    "        parsed_output = output['data']\n",
    "    except Exception as e:\n",
    "        print(f\"Error during classification: {str(e)}\")\n",
    "        parsed_output = {}\n",
    "\n",
    "    # Extract the classification result\n",
    "    if parsed_output and 'scene_classification' in parsed_output and parsed_output['scene_classification']:\n",
    "        classification = parsed_output['scene_classification']\n",
    "        if isinstance(classification, list) and len(classification) > 0:\n",
    "            classification = classification[0]\n",
    "        scene['dramatic_element'] = classification.get('dramatic_element', 'Unclassified')\n",
    "        scene['justification'] = classification.get('justification', 'No justification provided')\n",
    "        scene['scene_description'] = classification.get('scene_description', 'No description provided')\n",
    "        scene['scene_title'] = classification.get('scene_title', 'No title provided')\n",
    "        scene['scene_type'] = classification.get('scene_type', 'Unknown')\n",
    "    else:\n",
    "        scene['dramatic_element'] = \"Unclassified\"\n",
    "        scene['justification'] = \"Classification failed or returned empty result\"\n",
    "        scene['scene_description'] = \"No description provided.\"\n",
    "        scene['scene_title'] = \"No title provided.\"\n",
    "        scene['scene_type'] = \"Unknown\"\n",
    "\n",
    "    try:\n",
    "        # Prepare the text from eventstream\n",
    "        eventstream_text = \"\\n\".join([f\"{event.get('character', '')} {event.get('dialogue', '')}\" for event in scene['events']])\n",
    "        documents = [Document(page_content=eventstream_text)]\n",
    "        graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "        # Access the nodes and relationships directly from the GraphDocument\n",
    "        nodes = graph_documents[0].nodes\n",
    "        relationships = graph_documents[0].relationships\n",
    "\n",
    "        # Convert nodes and relationships to a dictionary representation\n",
    "        graph_dict = {\n",
    "            \"nodes\": [{\"id\": make_serializable(node.id),\n",
    "                    \"type\": make_serializable(node.type),\n",
    "                    \"properties\": make_serializable(node.properties)} for node in nodes],\n",
    "            \"links\": [{\"source\": make_serializable(rel.source.id),\n",
    "                    \"target\": make_serializable(rel.target.id),\n",
    "                    \"type\": make_serializable(rel.type),\n",
    "                    \"properties\": make_serializable(rel.properties)} for rel in relationships]\n",
    "        }\n",
    "        scene['scene_knowledge_graph'] = graph_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during knowledge graph generation: {str(e)}\")\n",
    "        scene['scene_knowledge_graph'] = {\"error\": f\"Knowledge graph generation failed: {str(e)}\"}\n",
    "\n",
    "    return scene\n",
    "\n",
    "\n",
    "def parse_faust_text(file_path):\n",
    "    with io.open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    lines = text.splitlines()\n",
    "    lines = lines[0:5000]\n",
    "    \n",
    "    structured_data = []\n",
    "    act = None\n",
    "    scene = None\n",
    "    previous_scenes = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        line = lines[i].rstrip('\\n')\n",
    "        line_stripped = line.lstrip()\n",
    "\n",
    "        # Detect Act\n",
    "        act_match = re.match(r'^ACT\\s*([IVX]+)\\.?\\s*$', line_stripped, re.IGNORECASE)\n",
    "        if act_match:\n",
    "            # Classify the previous scene if it exists\n",
    "            if scene and scene['events']:\n",
    "                scene = incremental_classify_scene(scene, dramatic_structure_dict, previous_scenes)\n",
    "                previous_scenes.append(scene)\n",
    "                act['scenes'].append(scene)\n",
    "            act_number = act_match.group(1)\n",
    "            act_sequence_number = roman_to_int(act_number)\n",
    "            act = {'act_node_id': act_number, 'act_sequence_number': act_sequence_number, 'scenes': []}\n",
    "            structured_data.append(act)\n",
    "            scene = None\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Detect Scene\n",
    "        scene_match = re.match(r'^Scene ([IVX]+)\\.$', line_stripped, re.IGNORECASE)\n",
    "        if scene_match:\n",
    "            # Classify the previous scene if it exists\n",
    "            if scene and scene['events']:\n",
    "                scene = incremental_classify_scene(scene, dramatic_structure_dict, previous_scenes)\n",
    "                previous_scenes.append(scene)\n",
    "                if act:\n",
    "                    act['scenes'].append(scene)\n",
    "            scene_number = scene_match.group(1)\n",
    "            if act is None:\n",
    "                act = {'act_node_id': 'Unknown', 'act_sequence_number': 0, 'scenes': []}\n",
    "                structured_data.append(act)\n",
    "            scene_sequence_number = act['act_sequence_number'] + roman_to_decimal(scene_number)\n",
    "            scene = {\n",
    "                'scene_node_id': scene_number,\n",
    "                'scene_sequence_number': scene_sequence_number,\n",
    "                'events': []\n",
    "            }\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Handle Empty Line\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Detect Character Name with possible stage direction\n",
    "        char_match = re.match(r'^\\s([^\\s_].*?)\\.(\\s*_\\w.*?_)?$', line)\n",
    "        if char_match:\n",
    "            character = char_match.group(1).strip().replace('.', '')\n",
    "            act_description = char_match.group(2).replace('[_', '').replace('_]', '').strip() if char_match.group(2) else ''\n",
    "            i += 1\n",
    "            dialogue = []\n",
    "            while i < len(lines) and lines[i].strip() != '' and lines[i].startswith(' '):\n",
    "                dialogue.append(lines[i].strip())\n",
    "                i += 1\n",
    "            dialogue = ' '.join(dialogue)\n",
    "            event_sequence_number = generate_event_sequence_number(act['act_sequence_number'], scene['scene_node_id'], len(scene['events']) + 1)\n",
    "            event = {\n",
    "                'character': character + '.',\n",
    "                'dialogue': dialogue,\n",
    "                'event_sequence_number': event_sequence_number\n",
    "            }\n",
    "            if act_description:\n",
    "                event['action'] = act_description\n",
    "            if scene is None:\n",
    "                scene = {\n",
    "                    'scene_node_id': 'Unknown',\n",
    "                    'scene_sequence_number': 0.0,\n",
    "                    'events': []\n",
    "                }\n",
    "                if act is None:\n",
    "                    act = {'act_node_id': 'Unknown', 'act_sequence_number': 0, 'scenes': []}\n",
    "                    structured_data.append(act)\n",
    "                act['scenes'].append(scene)\n",
    "            scene['events'].append(event)\n",
    "            continue\n",
    "\n",
    "        # Detect Stage Direction outside character's dialogue\n",
    "        stage_direction_match = re.match(r'^\\[(.*)\\]$', line.strip())\n",
    "        if stage_direction_match:\n",
    "            stage_direction = stage_direction_match.group(1).strip()\n",
    "            event_sequence_number = generate_event_sequence_number(act['act_sequence_number'], scene['scene_node_id'], len(scene['events']) + 1)\n",
    "            event = {\n",
    "                'stage_direction': stage_direction,\n",
    "                'event_sequence_number': event_sequence_number\n",
    "            }\n",
    "            if scene is None:\n",
    "                scene = {\n",
    "                    'scene_node_id': 'Unknown',\n",
    "                    'scene_sequence_number': 0.0,\n",
    "                    'events': []\n",
    "                }\n",
    "                if act is None:\n",
    "                    act = {'act_node_id': 'Unknown', 'act_sequence_number': 0, 'scenes': []}\n",
    "                    structured_data.append(act)\n",
    "                act['scenes'].append(scene)\n",
    "            scene['events'].append(event)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Move to next line if none of the above matched\n",
    "        i += 1\n",
    "\n",
    "# Classify the last scene if it exists\n",
    "    if scene and scene['events']:\n",
    "        scene = incremental_classify_scene(scene, dramatic_structure_dict, previous_scenes)\n",
    "        previous_scenes.append(scene)\n",
    "        act['scenes'].append(scene)\n",
    "\n",
    "    return json.dumps(structured_data, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (str, int, float, bool)) or obj is None:\n",
    "        return obj\n",
    "    elif hasattr(obj, '__dict'):\n",
    "        return make_serializable(obj.__dict__)\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "\n",
    "def generate_event_sequence_number(act_seq_num, scene_id, event_index):\n",
    "    scene_decimal = roman_to_decimal(scene_id)\n",
    "    return f\"{act_seq_num}.{int(scene_decimal)}.{event_index}\"\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        chunk_text = ' '.join(chunk_words)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "def build_graph_dataframes(structured_data_json):\n",
    "    structured_data = json.loads(structured_data_json)\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    kg_nodes = []\n",
    "    kg_edges = []\n",
    "\n",
    "    previous_act_id = None\n",
    "\n",
    "    for act_index, act in enumerate(structured_data):\n",
    "        act_id = f\"Act_{act_index+1}\"\n",
    "        act_label = 'Act'\n",
    "        act_properties = {\n",
    "            'act_node_id': act.get('act_node_id', ''),\n",
    "            'act_sequence_number': act.get('act_sequence_number', '')\n",
    "        }\n",
    "        nodes.append({\n",
    "            'id': act_id,\n",
    "            'label': act_label,\n",
    "            'properties': act_properties\n",
    "        })\n",
    "\n",
    "        # NEXT relationship between acts\n",
    "        if previous_act_id is not None:\n",
    "            edges.append({\n",
    "                'source': previous_act_id,\n",
    "                'target': act_id,\n",
    "                'type': 'NEXT',\n",
    "                'properties': {}\n",
    "            })\n",
    "        previous_act_id = act_id\n",
    "\n",
    "        previous_scene_id = None\n",
    "        scenes = act.get('scenes', [])\n",
    "        for scene_index, scene in enumerate(scenes):\n",
    "            scene_id = f\"{act_id}_Scene_{scene_index+1}\"\n",
    "            scene_label = 'Scene'\n",
    "            scene_properties = {\n",
    "                'scene_node_id': scene.get('scene_node_id', ''),\n",
    "                'scene_sequence_number': scene.get('scene_sequence_number', ''),\n",
    "                'dramatic_element': scene.get('dramatic_element', ''),\n",
    "                'justification': scene.get('justification', ''),\n",
    "                'scene_description': scene.get('scene_description', ''),\n",
    "                'scene_title': scene.get('scene_title', ''),\n",
    "                'scene_type': scene.get('scene_type', '')\n",
    "            }\n",
    "            nodes.append({\n",
    "                'id': scene_id,\n",
    "                'label': scene_label,\n",
    "                'properties': scene_properties\n",
    "            })\n",
    "\n",
    "            # PART_OF relationship between scene and act\n",
    "            edges.append({\n",
    "                'source': scene_id,\n",
    "                'target': act_id,\n",
    "                'type': 'PART_OF',\n",
    "                'properties': {}\n",
    "            })\n",
    "\n",
    "            # NEXT relationship between scenes\n",
    "            if previous_scene_id is not None:\n",
    "                edges.append({\n",
    "                    'source': previous_scene_id,\n",
    "                    'target': scene_id,\n",
    "                    'type': 'NEXT',\n",
    "                    'properties': {}\n",
    "                })\n",
    "            previous_scene_id = scene_id\n",
    "\n",
    "            previous_event_id = None\n",
    "            events = scene.get('events', [])\n",
    "            for event_index, event in enumerate(events):\n",
    "                event_id = f\"{scene_id}_Event_{event_index+1}\"\n",
    "                event_label = 'Event'\n",
    "                event_properties = event.copy()\n",
    "                nodes.append({\n",
    "                    'id': event_id,\n",
    "                    'label': event_label,\n",
    "                    'properties': event_properties\n",
    "                })\n",
    "\n",
    "                # PART_OF relationship between event and scene\n",
    "                edges.append({\n",
    "                    'source': event_id,\n",
    "                    'target': scene_id,\n",
    "                    'type': 'PART_OF',\n",
    "                    'properties': {}\n",
    "                })\n",
    "\n",
    "                # NEXT relationship between events\n",
    "                if previous_event_id is not None:\n",
    "                    edges.append({\n",
    "                        'source': previous_event_id,\n",
    "                        'target': event_id,\n",
    "                        'type': 'NEXT',\n",
    "                        'properties': {}\n",
    "                    })\n",
    "                previous_event_id = event_id\n",
    "\n",
    "            # Now process chunks and knowledge graph\n",
    "            eventstream_text = \"\\n\".join([f\"{event.get('character', '')} {event.get('dialogue', '')}\" for event in scene['events']])\n",
    "            chunks = split_text_into_chunks(eventstream_text, chunk_size=500)\n",
    "            previous_chunk_id = None\n",
    "            for chunk_index, chunk_text in enumerate(chunks):\n",
    "                chunk_id = f\"{scene_id}_Chunk_{chunk_index+1}\"\n",
    "                chunk_label = 'Chunk'\n",
    "                chunk_properties = {\n",
    "                    'text': chunk_text\n",
    "                }\n",
    "                nodes.append({\n",
    "                    'id': chunk_id,\n",
    "                    'label': chunk_label,\n",
    "                    'properties': chunk_properties\n",
    "                })\n",
    "\n",
    "                # PART_OF relationship between chunk and scene\n",
    "                edges.append({\n",
    "                    'source': chunk_id,\n",
    "                    'target': scene_id,\n",
    "                    'type': 'PART_OF',\n",
    "                    'properties': {}\n",
    "                })\n",
    "\n",
    "                # NEXT relationship between chunks\n",
    "                if previous_chunk_id is not None:\n",
    "                    edges.append({\n",
    "                        'source': previous_chunk_id,\n",
    "                        'target': chunk_id,\n",
    "                        'type': 'NEXT',\n",
    "                        'properties': {}\n",
    "                    })\n",
    "                previous_chunk_id = chunk_id\n",
    "\n",
    "                # Run LLMGraphTransformer on the chunk\n",
    "                try:\n",
    "                    documents = [Document(page_content=chunk_text)]\n",
    "                    graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "                    # Extract nodes and relationships from graph_documents\n",
    "                    for graph_doc in graph_documents:\n",
    "                        for kg_node in graph_doc.nodes:\n",
    "                            kg_node_id = f\"{chunk_id}_{kg_node.id}\"\n",
    "                            kg_node_label = kg_node.type\n",
    "                            kg_node_properties = kg_node.properties.copy()  # Create a copy of the properties\n",
    "                            kg_node_properties['chunk_id'] = chunk_id\n",
    "                            kg_node_properties['scene_id'] = scene_id\n",
    "                            kg_node_properties['entity'] = kg_node.id  # Store the original entity name\n",
    "                            kg_nodes.append({\n",
    "                                'id': kg_node_id,\n",
    "                                'entity': kg_node.id,\n",
    "                                'label': kg_node_label,\n",
    "                                'properties': kg_node_properties\n",
    "                            })\n",
    "\n",
    "                            # HAS_ENTITY relationship between chunk and entity\n",
    "                            kg_edges.append({\n",
    "                                'source': chunk_id,\n",
    "                                'target': kg_node_id,\n",
    "                                'type': 'HAS_ENTITY',\n",
    "                                'properties': {}\n",
    "                            })\n",
    "                        for kg_rel in graph_doc.relationships:\n",
    "                            kg_edge_source = f\"{chunk_id}_{kg_rel.source.id}\"\n",
    "                            kg_edge_target = f\"{chunk_id}_{kg_rel.target.id}\"\n",
    "                            kg_edges.append({\n",
    "                                'source': kg_edge_source,\n",
    "                                'target': kg_edge_target,\n",
    "                                'type': kg_rel.type,\n",
    "                                'properties': kg_rel.properties\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during knowledge graph generation for chunk: {str(e)}\")\n",
    "\n",
    "    # Create DataFrames\n",
    "    nodes_df = pd.DataFrame(nodes)\n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    kg_nodes_df = pd.DataFrame(kg_nodes)\n",
    "    kg_edges_df = pd.DataFrame(kg_edges)\n",
    "\n",
    "    return nodes_df, edges_df, kg_nodes_df, kg_edges_df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def expand_properties(df):\n",
    "    # Extract the properties column\n",
    "    properties_df = pd.json_normalize(df['properties'])\n",
    "    \n",
    "    # Drop the original properties column\n",
    "    df = df.drop('properties', axis=1)\n",
    "    \n",
    "    # Concatenate the original DataFrame with the expanded properties\n",
    "    return pd.concat([df, properties_df], axis=1)\n",
    "\n",
    "# Now, parse the text and build the DataFrames\n",
    "structured_data_json = parse_faust_text('faust.txt')\n",
    "nodes_df, edges_df, kg_nodes_df, kg_edges_df = build_graph_dataframes(structured_data_json)\n",
    "\n",
    "# Expand properties for each DataFrame\n",
    "nodes_df = expand_properties(nodes_df)\n",
    "edges_df = expand_properties(edges_df)\n",
    "kg_nodes_df = expand_properties(kg_nodes_df)\n",
    "kg_edges_df = expand_properties(kg_edges_df)\n",
    "\n",
    "# Save the DataFrames to CSV or any preferred format\n",
    "nodes_df.to_csv('nodes.csv', index=False)\n",
    "edges_df.to_csv('edges.csv', index=False)\n",
    "kg_nodes_df.to_csv('kg_nodes.csv', index=False)\n",
    "kg_edges_df.to_csv('kg_edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error adding kg edge between Act_2_Scene_3_Chunk_2_Wagner and Act_2_Scene_3_Chunk_2_Hours Of Whim\n",
      "Error adding kg edge between Act_2_Scene_3_Chunk_2_Wagner and Act_2_Scene_3_Chunk_2_Reason\n",
      "Error adding kg edge between Act_2_Scene_3_Chunk_2_Wagner and Act_2_Scene_3_Chunk_2_Forest\n",
      "Error adding kg edge between Act_2_Scene_3_Chunk_2_Wagner and Act_2_Scene_3_Chunk_2_Field\n",
      "Error adding kg edge between Act_2_Scene_3_Chunk_2_Wagner and Act_2_Scene_3_Chunk_2_Eye\n",
      "Error adding kg edge between Act_2_Scene_3_Chunk_2_Wagner and Act_2_Scene_3_Chunk_2_Man\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Naples\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Malta\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Turkish Ship\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Bullion\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Sultan\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Heaven\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Grave\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Dung\n",
      "Error adding kg edge between Act_3_Scene_7_Chunk_2_John Doe and Act_3_Scene_7_Chunk_2_Mouldy Straw\n",
      "Error adding kg edge between Act_4_Scene_3_Chunk_3_Balaam and Act_4_Scene_3_Chunk_3_Ass\n",
      "Error adding kg edge between Act_4_Scene_3_Chunk_3_God and Act_4_Scene_3_Chunk_3_Pass\n",
      "Error adding kg edge between Act_4_Scene_3_Chunk_3_Devil and Act_4_Scene_3_Chunk_3_Earth\n",
      "Error adding kg edge between Act_4_Scene_4_Chunk_1_Margaret and Act_4_Scene_4_Chunk_1_Peace\n",
      "Error adding kg edge between Act_4_Scene_4_Chunk_1_Margaret and Act_4_Scene_4_Chunk_1_Sore\n",
      "Error adding kg edge between Act_4_Scene_5_Chunk_1_Faust and Act_4_Scene_5_Chunk_1_God\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "\n",
    "def create_pyvis_network(nodes_df, edges_df, kg_nodes_df, kg_edges_df):\n",
    "    # Create a Pyvis network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "\n",
    "    # Add nodes\n",
    "    for _, row in nodes_df.iterrows():\n",
    "        node_properties = row.dropna().to_dict()\n",
    "        node_id = node_properties.pop('id')\n",
    "        node_label = node_properties.pop('label', 'Unknown')\n",
    "\n",
    "        # For Events, include the character value in the label\n",
    "        if node_label == 'Event' and 'character' in node_properties:\n",
    "            node_label = f\"Event: {node_properties['character']}\"\n",
    "\n",
    "        try:\n",
    "            net.add_node(node_id, label=node_label, title=str(node_properties), color=get_color(node_label))\n",
    "        except:\n",
    "            print(f\"Error adding node: {node_id}\")\n",
    "\n",
    "    # Add edges\n",
    "    for _, row in edges_df.iterrows():\n",
    "        source = row['source']\n",
    "        target = row['target']\n",
    "        edge_type = row['type']\n",
    "\n",
    "        # Add the relationship type as a label on the edge\n",
    "        try:\n",
    "            net.add_edge(source, target, title=edge_type, label=edge_type, font={'size': 6})\n",
    "        except:\n",
    "            print(f\"Error adding edge between {source} and {target}\")\n",
    "\n",
    "    # Add knowledge graph nodes\n",
    "    for _, row in kg_nodes_df.iterrows():\n",
    "        kg_node_properties = row.dropna().to_dict()\n",
    "        kg_node_id = kg_node_properties.pop('id')\n",
    "        kg_node_label = kg_node_properties.pop('label', 'KG_Unknown')\n",
    "\n",
    "        # Include the entity value in the label for knowledge graph nodes\n",
    "        if 'entity' in kg_node_properties:\n",
    "            kg_node_label = f\"{kg_node_label}: {kg_node_properties['entity']}\"\n",
    "\n",
    "        try:\n",
    "            net.add_node(kg_node_id, label=kg_node_label, title=str(kg_node_properties), color=get_color(kg_node_label))\n",
    "        except:\n",
    "            print(f\"Error adding kg node: {kg_node_id}\")\n",
    "\n",
    "    # Add knowledge graph edges\n",
    "    for _, row in kg_edges_df.iterrows():\n",
    "        kg_source = row['source']\n",
    "        kg_target = row['target']\n",
    "        kg_edge_type = row['type']\n",
    "\n",
    "        # Add the relationship type as a label on the edge\n",
    "        try:\n",
    "            net.add_edge(kg_source, kg_target, title=kg_edge_type, label=kg_edge_type, font={'size': 6})\n",
    "        except:\n",
    "            print(f\"Error adding kg edge between {kg_source} and {kg_target}\")\n",
    "\n",
    "    # Connect knowledge graph nodes to chunk nodes\n",
    "    for _, row in kg_nodes_df.iterrows():\n",
    "        kg_node_id = row['id']\n",
    "        chunk_id = row['chunk_id']\n",
    "\n",
    "        if chunk_id in net.get_nodes():\n",
    "            try:\n",
    "                net.add_edge(chunk_id, kg_node_id, title='HAS_ENTITY', label='HAS_ENTITY', font={'size': 6})\n",
    "            except:\n",
    "                print(f\"Error connecting kg node {kg_node_id} to chunk node {chunk_id}\")\n",
    "\n",
    "    # Set global options for edge labels\n",
    "    net.set_edge_smooth('dynamic')  # This can help with label visibility\n",
    "\n",
    "    return net\n",
    "\n",
    "def get_color(node_type):\n",
    "    color_map = {\n",
    "        'Act': '#FF9999',\n",
    "        'Scene': '#66B2FF',\n",
    "        'Event': '#99FF99',\n",
    "        'Chunk': '#FFCC99',\n",
    "        'Entity': '#FF99FF'\n",
    "    }\n",
    "    return color_map.get(node_type, '#FFFFFF')\n",
    "\n",
    "# Create and visualize the network\n",
    "net = create_pyvis_network(nodes_df, edges_df, kg_nodes_df, kg_edges_df)\n",
    "\n",
    "# Set some display options\n",
    "net.toggle_physics(True)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "\n",
    "# Save and show the network\n",
    "net.save_graph(\"faust_network.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('go for it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEO4J GDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "### User\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "### GDB Insatnce\n",
    "# Neo4j connection setup\n",
    "# NEO4J_URI = \"neo4j+ssc://f3e13232.databases.neo4j.io\"\n",
    "# NEO4J_PASSWORD = \"7XIfeMdlGqzbzpBOCoePwWMSJltaHP4L598VjqjwXbE\"\n",
    "\n",
    "### GDS Instance\n",
    "NEO4J_URI = \"neo4j+s://db6a9f9c.databases.neo4j.io\"\n",
    "NEO4J_PASSWORD = \"5G3vK13O9qdE01F5R270JzldVIzYThrCxeVxJaMFf4c\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "gds = GraphDataScience(NEO4J_URI, auth=(\"neo4j\", NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read from defunct connection IPv4Address(('db6a9f9c.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('35.240.86.240', 7687)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_1 -> Mother: Failed to read from defunct connection IPv4Address(('db6a9f9c.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('35.240.86.240', 7687)))\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_1 -> Father: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_1 -> Brother: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_1 -> Sister: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_2 -> Margaret: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_2 -> Faust: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_2 -> Martha: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_2 -> Mephistopheles: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_2 -> The Infant: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_2 -> The Elf: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_3 -> Faust: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_3 -> Margaret: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_3 -> Mephistopheles: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_1_Chunk_3 -> Martha: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_2_Chunk_1 -> Margaret: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_2_Chunk_1 -> Mephistopheles: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_2_Chunk_1 -> Faust: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_2_Chunk_1 -> Martha: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_1 -> Mephistopheles: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_1 -> Faust: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_2 -> Faust: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_2 -> Mephistopheles: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_2 -> The Lovely Maid: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_2 -> The Lord: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_2 -> The God: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_2 -> The Human: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_3 -> Road: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_3 -> He: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_3 -> Balaam: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_3 -> God: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_3 -> Man: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_3_Chunk_3 -> Devil: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_4_Chunk_1 -> Margaret: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_4_Chunk_1 -> Rest: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_4_Chunk_1 -> Heart: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_5_Chunk_1 -> Margaret: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_5_Chunk_1 -> Faust: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_5_Chunk_1 -> Henry: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_5_Chunk_1 -> Parson: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_5_Chunk_2 -> Faust: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_5_Chunk_2 -> Margaret: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_ENTITY connection Act_4_Scene_5_Chunk_2 -> Henry: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error creating HAS_DIALOGUE connections: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Error merging Entity nodes: Cannot resolve address db6a9f9c.databases.neo4j.io:7687\n",
      "Knowledge graph upload to Neo4j completed!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from neo4j.exceptions import CypherSyntaxError\n",
    "import pandas as pd\n",
    "\n",
    "def upload_to_neo4j(nodes_df, edges_df, kg_nodes_df, kg_edges_df):\n",
    "    kg_nodes_df = kg_nodes_df.loc[:, ~kg_nodes_df.columns.duplicated()]\n",
    "    kg_edges_df = kg_edges_df.loc[:, ~kg_edges_df.columns.duplicated()]\n",
    "    \n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        try:\n",
    "            session.run('MATCH (n) DETACH DELETE n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing database: {str(e)}\")\n",
    "\n",
    "        # Upload non-KG nodes\n",
    "        for _, row in nodes_df.iterrows():\n",
    "            try:\n",
    "                label = re.sub(r'\\W+', '_', row['label'])\n",
    "                node_id = row['id']\n",
    "                properties = {k: v for k, v in row.items() if k not in ['id', 'label'] and pd.notna(v)}\n",
    "                cypher_query = (\n",
    "                    f\"MERGE (n:`{label}` {{id: $node_id}}) \"\n",
    "                    \"SET n += $properties \"\n",
    "                    \"RETURN n\"\n",
    "                )\n",
    "                session.run(cypher_query, node_id=node_id, properties=properties)\n",
    "            except CypherSyntaxError as e:\n",
    "                print(f\"Cypher syntax error for node {node_id}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading node {node_id}: {str(e)}\")\n",
    "\n",
    "        # Upload KG nodes\n",
    "        for _, row in kg_nodes_df.iterrows():\n",
    "            try:\n",
    "                label = row['label']\n",
    "                entity_name = row['entity'][0] if isinstance(row['entity'], list) else row['entity']\n",
    "                \n",
    "                # First MERGE the base Entity node\n",
    "                base_query = (\n",
    "                    \"MERGE (n:Entity {name: $entity_name}) \"\n",
    "                    \"RETURN n\"\n",
    "                )\n",
    "                session.run(base_query, entity_name=entity_name)\n",
    "                \n",
    "                # Then add the additional label\n",
    "                label_query = (\n",
    "                    \"MATCH (n:Entity {name: $entity_name}) \"\n",
    "                    f\"SET n:`{label}` \"\n",
    "                    \"RETURN n\"\n",
    "                )\n",
    "                session.run(label_query, entity_name=entity_name)\n",
    "                \n",
    "            except CypherSyntaxError as e:\n",
    "                print(f\"Cypher syntax error for KG node {entity_name}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading KG node {entity_name}: {str(e)}\")\n",
    "\n",
    "        # Upload non-KG edges\n",
    "        for _, row in edges_df.iterrows():\n",
    "            try:\n",
    "                properties = {k: v for k, v in row.items() if k not in ['source', 'target', 'type'] and pd.notna(v)}\n",
    "                relationship_type = re.sub(r'\\W+', '_', row['type'])\n",
    "                source_id = row['source']\n",
    "                target_id = row['target']\n",
    "                cypher_query = (\n",
    "                    \"MATCH (source {id: $source_id}) \"\n",
    "                    \"MATCH (target {id: $target_id}) \"\n",
    "                    f\"MERGE (source)-[r:`{relationship_type}`]->(target) \"\n",
    "                    \"SET r += $properties \"\n",
    "                    \"RETURN r\"\n",
    "                )\n",
    "                session.run(cypher_query, source_id=source_id, target_id=target_id, properties=properties)\n",
    "            except CypherSyntaxError as e:\n",
    "                print(f\"Cypher syntax error for edge {source_id} -> {target_id}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading edge {source_id} -> {target_id}: {str(e)}\")\n",
    "\n",
    "        # Upload KG edges\n",
    "        for _, row in kg_edges_df.iterrows():\n",
    "            try:\n",
    "                properties = {k: v for k, v in row.items() if k not in ['source', 'target', 'type'] and pd.notna(v)}\n",
    "                relationship_type = re.sub(r'\\W+', '_', row['type'])\n",
    "                source_name = row['source'].split('_')[-1] if isinstance(row['source'], str) else row['source'][0]\n",
    "                target_name = row['target'].split('_')[-1] if isinstance(row['target'], str) else row['target'][0]\n",
    "                cypher_query = (\n",
    "                    \"MATCH (source:Entity {name: $source_name}) \"\n",
    "                    \"MATCH (target:Entity {name: $target_name}) \"\n",
    "                    f\"MERGE (source)-[r:`{relationship_type}`]->(target) \"\n",
    "                    \"SET r += $properties \"\n",
    "                    \"RETURN r\"\n",
    "                )\n",
    "                session.run(cypher_query, source_name=source_name, target_name=target_name, properties=properties)\n",
    "            except CypherSyntaxError as e:\n",
    "                print(f\"Cypher syntax error for KG edge {source_name} -> {target_name}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading KG edge {source_name} -> {target_name}: {str(e)}\")\n",
    "\n",
    "        # Create HAS_ENTITY connections\n",
    "        for _, row in kg_nodes_df.iterrows():\n",
    "            try:\n",
    "                chunk_id = row['chunk_id']\n",
    "                entity_name = row['entity'][0] if isinstance(row['entity'], list) else row['entity']\n",
    "                node_id = row['id']\n",
    "                cypher_query = (\n",
    "                    \"MATCH (chunk {id: $chunk_id}) \"\n",
    "                    \"MATCH (entity:Entity {name: $entity_name}) \"\n",
    "                    \"MERGE (chunk)-[r:HAS_ENTITY]->(entity) \"\n",
    "                    \"SET r.id = $node_id \"\n",
    "                    \"RETURN r\"\n",
    "                )\n",
    "                session.run(cypher_query, chunk_id=chunk_id, entity_name=entity_name, node_id=node_id)\n",
    "            except CypherSyntaxError as e:\n",
    "                print(f\"Cypher syntax error for HAS_ENTITY connection {chunk_id} -> {entity_name}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating HAS_ENTITY connection {chunk_id} -> {entity_name}: {str(e)}\")\n",
    "        \n",
    "\n",
    "        # Create HAS_DIALOGUE connections between Event and Entity nodes\n",
    "        try:\n",
    "            cypher_query = \"\"\"\n",
    "            MATCH (e:Event)\n",
    "            WHERE e.character IS NOT NULL\n",
    "            WITH e, trim(replace(e.character, '.', '')) AS trimmed_character\n",
    "            MATCH (entity:Entity)\n",
    "            WHERE entity.name = trimmed_character\n",
    "            MERGE (e)-[r:HAS_DIALOGUE]->(entity)\n",
    "            RETURN count(r) as created_relationships\n",
    "            \"\"\"\n",
    "            result = session.run(cypher_query)\n",
    "            created_relationships = result.single()['created_relationships']\n",
    "            print(f\"Created {created_relationships} HAS_DIALOGUE relationships\")\n",
    "        except CypherSyntaxError as e:\n",
    "            print(f\"Cypher syntax error for HAS_DIALOGUE connections: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating HAS_DIALOGUE connections: {str(e)}\")\n",
    "\n",
    "        try:\n",
    "            cypher_query = \"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WITH e.name AS name, collect(e) AS nodes\n",
    "            WHERE size(nodes) > 1\n",
    "            CALL apoc.refactor.mergeNodes(nodes, {properties:\"combine\", mergeRels:true})\n",
    "            YIELD node\n",
    "            RETURN node\n",
    "            \"\"\"\n",
    "            session.run(cypher_query)\n",
    "        except CypherSyntaxError as e:\n",
    "            print(f\"Cypher syntax error for merging Entity nodes: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging Entity nodes: {str(e)}\")\n",
    "            \n",
    "\n",
    "    print(\"Knowledge graph upload to Neo4j completed!\")\n",
    "\n",
    "nodes_df = pd.read_csv('nodes.csv')\n",
    "edges_df = pd.read_csv('edges.csv')\n",
    "kg_nodes_df = pd.read_csv('kg_nodes.csv')\n",
    "kg_edges_df = pd.read_csv('kg_edges.csv')\n",
    "\n",
    "# Call the function once with all dataframes\n",
    "upload_to_neo4j(nodes_df, edges_df, kg_nodes_df, kg_edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53541638c1e4ecc81c15f5bf465a6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "def showGraph():\n",
    "    session = driver.session()\n",
    "    w = GraphWidget(graph = session.run(\"MATCH (n)-[r]->(m) RETURN n, r, m LIMIT 1000\").graph())\n",
    "    def custom_node_label_mapping(node: dict):\n",
    "        # print(f\"Node data: {node}\")  # Debug print\n",
    "        properties = node.get('properties', {})\n",
    "        label = properties.get('label', '')\n",
    "        # print(f\"Label: {label}, Properties: {properties}\")  # Debug print\n",
    "        \n",
    "        if 'Entity' in label:\n",
    "            return {properties.get('name', 'Unknown')}\n",
    "        elif 'Event' in label:\n",
    "            return {properties.get('character', 'Unknown')}\n",
    "        elif 'Act' in label:\n",
    "            return {properties.get('act_node_id', 'Unknown')}\n",
    "        elif 'Scene' in label:\n",
    "            return {properties.get('id', '').split('_')[-1]}\n",
    "        elif 'Chunk' in label:\n",
    "            return {properties.get('id', 'Unknown')}\n",
    "        \n",
    "        # Default case if no matching label\n",
    "        return f\"Unknown ({label})\"\n",
    "    w.set_node_label_mapping(custom_node_label_mapping)\n",
    "    \n",
    "    return w\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to write data to connection ResolvedIPv4Address(('35.240.86.240', 7687)) (ResolvedIPv4Address(('35.240.86.240', 7687)))\n",
      "Failed to write data to connection IPv4Address(('db6a9f9c.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('35.240.86.240', 7687)))\n",
      "Failed to write data to connection ResolvedIPv4Address(('35.240.86.240', 7687)) (ResolvedIPv4Address(('35.240.86.240', 7687)))\n",
      "Failed to write data to connection IPv4Address(('db6a9f9c.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('35.240.86.240', 7687)))\n",
      "Unable to retrieve routing information\n",
      "Unable to connect to the Neo4j DBMS. Trying again...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component count: 15\n",
      "Component distribution: {'min': 1, 'p5': 1, 'max': 114, 'p999': 114, 'p99': 114, 'p1': 1, 'p10': 1, 'p90': 5, 'p50': 2, 'p25': 1, 'p75': 3, 'p95': 114, 'mean': 9.6}\n",
      "Centrality measures have been written to the graph\n",
      "Entities with communities property: [{'entity_count': 144}]\n",
      "Sample entity communities: [{'name': 'Faust', 'communities': [105, 23], 'e.degree_centrality': 110.0, 'e.betweenness_centrality': 1139.0, 'e.eigenvector_centrality': 4.1980812130071153e-07}]\n",
      "\n",
      "Counts at each step: [{'initial_count': 144, 'after_unwind': 288, 'after_community_creation': 288}]\n",
      "\n",
      "Chunk-Entity connections: [{'chunk_entity_connections': 179}]\n",
      "Community structure created: [{'entities_processed': 128, 'communities_created': 34, 'chunks_associated': 24}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url = NEO4J_URI,\n",
    "    username= NEO4J_USERNAME,\n",
    "    password= NEO4J_PASSWORD,\n",
    "    database= NEO4J_DATABASE\n",
    "    )\n",
    "\n",
    "exists_result = gds.graph.exists(\"communities\")\n",
    "if exists_result['exists'].item():  # Use item() instead of bool() for numpy.bool_\n",
    "    gds.graph.drop(\"communities\")\n",
    "\n",
    "G, result = gds.graph.project(\n",
    "    \"communities\",  #  Graph name\n",
    "    \"Entity\",  #  Node projection\n",
    "    {\n",
    "        \"_ALL_\": {\n",
    "            \"type\": \"*\",\n",
    "            \"orientation\": \"UNDIRECTED\",\n",
    "            \"properties\": {\"weight\": {\"property\": \"*\", \"aggregation\": \"COUNT\"}},\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "wcc = gds.wcc.stats(G)\n",
    "print(f\"Component count: {wcc['componentCount']}\")\n",
    "print(f\"Component distribution: {wcc['componentDistribution']}\")\n",
    "\n",
    "gds.leiden.write(\n",
    "    G,\n",
    "    writeProperty=\"communities\",\n",
    "    includeIntermediateCommunities=True,\n",
    "    relationshipWeightProperty=\"weight\",\n",
    ")\n",
    "\n",
    "gds.graph.drop(\"communities\")\n",
    "\n",
    "exists_result = gds.graph.exists(\"centrality_graph\")\n",
    "if exists_result['exists'].item():\n",
    "    gds.graph.drop(\"centrality_graph\")\n",
    "\n",
    "\n",
    "G, result = gds.graph.project(\n",
    "    \"centrality_graph\",  #  Graph name\n",
    "    \"Entity\",  #  Node projection\n",
    "    {\n",
    "        \"_ALL_\": {\n",
    "            \"type\": \"*\",\n",
    "            \"orientation\": \"NATURAL\",\n",
    "            \"properties\": {\n",
    "                \"weight\": {\n",
    "                    \"property\": \"*\",  # Changed from \"weight\" to \"*\" to count parallel relationships\n",
    "                    \"aggregation\": \"COUNT\"  # Changed from \"SINGLE\" to \"COUNT\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate and write degree centrality\n",
    "gds.degree.write(\n",
    "    G,\n",
    "    writeProperty=\"degree_centrality\",\n",
    "    relationshipWeightProperty=\"weight\"\n",
    ")\n",
    "\n",
    "# Calculate and write betweenness centrality\n",
    "gds.betweenness.write(\n",
    "    G,\n",
    "    writeProperty=\"betweenness_centrality\"\n",
    ")\n",
    "\n",
    "# Calculate and write eigenvector centrality\n",
    "gds.eigenvector.write(\n",
    "    G,\n",
    "    writeProperty=\"eigenvector_centrality\",\n",
    "    relationshipWeightProperty=\"weight\",\n",
    "    maxIterations=100\n",
    ")\n",
    "\n",
    "print(\"Centrality measures have been written to the graph\")\n",
    "\n",
    "# Optional: Drop the projected graph to free up memory\n",
    "gds.graph.drop(\"centrality_graph\")\n",
    "\n",
    "# First, let's check if we have entities with communities property\n",
    "result = graph.query(\"\"\"\n",
    "MATCH (e:Entity)\n",
    "WHERE e.communities IS NOT NULL\n",
    "RETURN count(e) as entity_count\n",
    "\"\"\")\n",
    "print(f\"Entities with communities property: {result}\")\n",
    "\n",
    "# First, let's check a sample entity\n",
    "result = graph.query(\"\"\"\n",
    "MATCH (e:Entity)\n",
    "WHERE e.communities IS NOT NULL\n",
    "RETURN e.name as name, e.communities as communities, e.degree_centrality, e.betweenness_centrality, e.eigenvector_centrality\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "print(\"Sample entity communities:\", result)\n",
    "\n",
    "# Now let's check the counts step by step\n",
    "result = graph.query(\"\"\"\n",
    "MATCH (e:Entity)\n",
    "WHERE e.communities IS NOT NULL\n",
    "WITH count(e) as initial_count\n",
    "MATCH (e:Entity)\n",
    "WHERE e.communities IS NOT NULL\n",
    "UNWIND range(0, size(e.communities) - 1) AS index\n",
    "WITH initial_count, count(*) as after_unwind\n",
    "MATCH (e:Entity)\n",
    "WHERE e.communities IS NOT NULL\n",
    "UNWIND range(0, size(e.communities) - 1) AS index\n",
    "WITH e, index, initial_count, after_unwind\n",
    "MERGE (c:Community {id: toString(index) + '-' + toString(e.communities[index])})\n",
    "WITH initial_count, after_unwind, count(*) as after_community_creation\n",
    "RETURN initial_count, after_unwind, after_community_creation\n",
    "\"\"\")\n",
    "print(\"\\nCounts at each step:\", result)\n",
    "\n",
    "# Check chunk connections\n",
    "result = graph.query(\"\"\"\n",
    "MATCH (chunk:Chunk)-[:HAS_ENTITY]->(e:Entity)\n",
    "WHERE e.communities IS NOT NULL\n",
    "RETURN count(*) as chunk_entity_connections\n",
    "\"\"\")\n",
    "print(\"\\nChunk-Entity connections:\", result)\n",
    "\n",
    "\n",
    "def create_community_structure(graph):\n",
    "    result = graph.query(\"\"\"\n",
    "    // First create communities and connect entities\n",
    "    MATCH (e:Entity)\n",
    "    WHERE e.communities IS NOT NULL\n",
    "    UNWIND e.communities AS community_id\n",
    "    MERGE (c:Community {id: community_id})\n",
    "    MERGE (e)-[:BELONGS_TO]->(c)\n",
    "    \n",
    "    // Connect chunks to communities \n",
    "    WITH DISTINCT e, c\n",
    "    MATCH (chunk:Chunk)-[:HAS_ENTITY]->(e)\n",
    "    MERGE (chunk)-[:ASSOCIATED_WITH]->(c)\n",
    "    \n",
    "    RETURN count(DISTINCT e) AS entities_processed,\n",
    "           count(DISTINCT c) AS communities_created,\n",
    "           count(DISTINCT chunk) AS chunks_associated\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"Community structure created: {result}\")\n",
    "\n",
    "# Call the function\n",
    "create_community_structure(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makos2tamas911\\AppData\\Local\\Temp\\ipykernel_11924\\513352512.py:153: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  with driver.session(database=NEO4J_DATABASE) as session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created and stored for 554 Event nodes and 50 Chunk nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to write data to connection ResolvedIPv4Address(('35.240.86.240', 7687)) (ResolvedIPv4Address(('35.240.86.240', 7687)))\n",
      "Failed to write data to connection IPv4Address(('db6a9f9c.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('35.240.86.240', 7687)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 similar Event nodes for query: 'A character's dialogue about a mysterious event'\n",
      "1. Communities:\n",
      "DialogueRelationships:\n",
      "Events:\n",
      "- You speak in riddles; the interpretation?\n",
      "- Sits now in restless mood, Knows neither what she would, nor what she should; Broods oer the trinkets night and day, And on him who sent them, more.\n",
      "- Who knows where the four winds have hurried it! A lady took him under her protection At Naples, as he wandered to and fro; She left him many a mark of her affection, As to his lifes end he had cause to know.\n",
      "Chunks:\n",
      "SharedEntities:\n",
      "\n",
      "\n",
      "Top 3 similar Chunk nodes for query: 'A character's dialogue about a mysterious event'\n",
      "1. Communities:\n",
      "- None\n",
      "- None\n",
      "- None\n",
      "Entities:\n",
      "- Faust\n",
      "- Wagner\n",
      "RelatedChunks:\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "- Related chunk shares entity: Faust\n",
      "CommunityChunks:\n",
      "Content:\n",
      "- Faust. The ice is now melted from stream and brook By the Springs genial life-giving look; Forth smiles young Hope in the greening vale, And ancient Winter, feeble and frail, Creeps cowering back to the mountains grey; And thence he sends, as he hies him away, Fitfullest brushes of icy hail, Sweeping the plain in his harmless flight. But the sun may brook no white, Everywhere stirs he the vegetive strife, Flushing the fields with the glow of life; But since few flowers yet deck the mead He takes him gay-dressed folk in their stead. Now from these heights I turn me back To view the citys busy track. Through the dark, deep-throated gate They are pouring and spreading in motley array. All sun themselves so blithe to-day. The Lords resurrection they celebrate, For that themselves to life are arisen. From lowly dwellings murky prison, From labour and business fetters tight, From the press of gables and roofs that meet Over the squeezing narrow street, From the churches solemn night Have they all been brought to the light. Lo! how nimbly the multitude Through the fields and the gardens hurry, How, in its breadth and length, the flood Wafts onward many a gleesome wherry, And this last skiff moves from the brink So laden that it seems to sink. Evn from the far hills winding way I the sunshine glitter their garments gay. I hear the hamlets noisy mirth; Here is the peoples heaven on earth, And great and small rejoice to-day. Here may I be a man, here dare The joys of men with men to share. Wagner. With you, Herr Doctor, one is proud to walk, Sharing your fame, improving by your talk; But, for myself, I shun the multitude, Being a foe to everything thats rude. I may not brook their senseless howling, Their fiddling, screaming, ninepin bowling; Like men possessed, they rave along, And call it joy, and call it song.\n",
      "- The fellows an outlaw! strike him down! [_They draw their knives and attack_ Mephistopheles. Altmayer. Where am I? in what lovely land? Frosch. Vineyards! can it be so? Siebel. And grapes too quite at hand! Brander. And here beneath this shady tree, This noble vine, these blushing clusters see! [_He vanishes with_ Faust. Siebel. Whats the matter? Altmayer. How now? Frosch. Was that your nose? Altmayer. It was a stroke shot through my every limb! Give me a chair!--I faint! My eyes grow dim! Frosch. Now tell me only what has been the matter? Siebel. Where is the fellow? Could I catch him here, His life out of his body I should batter! Altmayer. I saw him just this instant disappear, Riding upon a wine-cask--I declare I feel a weight like lead about my feet. [_Turning to the table._] I wonder if his d----d wine still be there! Siebel. Theres not a single drop; twas all a cheat. Frosch. And yet methinks that I was drinking wine. Brander. And I could swear I saw a clustered vine. Altmayer. Let none now say the age of miracles is past!\n",
      "- Wagner. Your pardon, sir, I heard your voice declaiming, No doubt some old Greek drama, and I came in, To profit by your learned recitation; For in these days the art of declamation Is held in highest estimation; And I have heard asserted that a preacher Might wisely have an actor for his teacher. Faust. Yes; when our parsons preach to make grimaces, As here and there a not uncommon case is. Wagner. Alack! when a poor wight is so confined Amid his books, shut up from all mankind, And sees the world scarce on a holiday, As through a telescope and far away, How may he hope, with nicely tempered skill, To bend the hearts he knows not to his will? Faust. What you dont feel, youll hunt to find in vain. It must gush from the soul, possess the brain, And with an instinct kindly force compel All captive hearts to own the grateful spell; Go to! sit oer your books, and snip and glue Your wretched piece-work, dressing your ragout From others feasts, your piteous flames still blowing From sparks beneath dull heaps of ashes glowing; Vain wonderment of children and of apes, If with such paltry meed content thou art; The human heart to heart he only shapes, Whose words flow warm from human heart to heart. Wagner. But the delivery is a chief concern In Rhetoric; and alas! here I have much to learn. Faust. Be thine to seek the honest gain, No shallow-tinkling fool! Sound sense finds utterance for itself, Without the critics rule. If clear your thought, and your intention true, What need to hunt for words with much ado? The trim orations your fine speaker weaves, Crisping light shreds of thought for shallow minds, Are unrefreshing as the foggy winds That whistle through the sapless autumn leaves. Wagner. Alas! how long is art, And human life how short! I feel at times with all my learned pains, As if a weight of lead were at my heart, And palsy on my brains. How high to climb up learnings lofty stair, How hard to find the helps that guide us there; And when scarce half the way behind him lies, His glass is run, and the poor devil dies! Faust. The parchment-roll is that the holy river, From which one draught shall slake the thirst for ever? The quickening power of science only he Can know, from whose own soul it gushes free. Wagner. And yet the spirit of a bygone age, To re-create may well the wise engage; To know the choicest thoughts of every ancient sage, And think how far above their best weve mounted high! Faust. O yes, I trow, even to the stars, so high! My friend, the ages that are past Are as a book with seven seals made fast; And what men call the spirit of the age, Is but the spirit of the gentlemen Who glass their own thoughts in the pliant page, And image back\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "\n",
    "# Configuration constants\n",
    "topChunks = 3\n",
    "topCommunities = 3\n",
    "topOutsideRels = 10\n",
    "topInsideRels = 10\n",
    "topEntities = 10\n",
    "\n",
    "# Modified Event retrieval query\n",
    "ev_retrieval_query = \"\"\"\n",
    "WITH collect(node) as nodes\n",
    "// Event-Chunk Mapping\n",
    "WITH nodes, collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n:Event)-[:HAS_ENTITY]->(e:Entity)<-[:HAS_ENTITY]-(c:Chunk)\n",
    "    WITH c, count(distinct n) as freq, n\n",
    "    // Add centrality-based ordering if specified\n",
    "    ORDER BY \n",
    "        CASE \n",
    "            WHEN $centrality_field IS NOT NULL THEN n[$centrality_field]\n",
    "            ELSE freq \n",
    "        END DESC,\n",
    "        freq DESC\n",
    "    LIMIT $topChunks\n",
    "    RETURN c.text AS chunkText\n",
    "} AS text_mapping,\n",
    "// Event-Community Mapping\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n:Event)-[:HAS_ENTITY]->(e:Entity)-[:BELONGS_TO]->(c:Community)\n",
    "    WITH c, c.community_rank as rank, c.weight as weight\n",
    "    RETURN 'Community ' + c.id + ' (rank: ' + toString(rank) + ', weight: ' + toString(weight) + ')' as summary\n",
    "    ORDER BY rank DESC, weight DESC\n",
    "    LIMIT $topCommunities\n",
    "} AS community_mapping,\n",
    "// Event-Entity Relationships\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n:Event)-[r:HAS_DIALOGUE]->(e:Entity)\n",
    "    WHERE NOT e IN nodes\n",
    "    RETURN 'Character ' + e.name + ' has dialogue in event' as descriptionText\n",
    "    LIMIT $topOutsideRels\n",
    "} as dialogue_rels,\n",
    "// Event-Event Relationships\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n:Event)-[:HAS_ENTITY]->(e:Entity)<-[:HAS_ENTITY]-(m:Event)\n",
    "    WHERE m IN nodes AND id(n) < id(m)\n",
    "    RETURN 'Events share entity: ' + e.name as descriptionText\n",
    "    LIMIT $topInsideRels\n",
    "} as shared_entity_rels,\n",
    "// Event descriptions\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    RETURN n.dialogue as descriptionText\n",
    "} as events\n",
    "RETURN {\n",
    "    Chunks: text_mapping,\n",
    "    Communities: community_mapping,\n",
    "    DialogueRelationships: dialogue_rels,\n",
    "    SharedEntities: shared_entity_rels,\n",
    "    Events: events\n",
    "} AS text, 1.0 AS score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "# Modified Chunk retrieval query\n",
    "cv_retrieval_query = \"\"\"\n",
    "WITH collect(node) as nodes\n",
    "// Chunk-Entity Mapping\n",
    "WITH nodes, collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n:Chunk)-[:HAS_ENTITY]->(e:Entity)\n",
    "    WITH e, count(distinct n) as freq, e.weight as weight, n\n",
    "    // Add centrality-based ordering if specified\n",
    "    ORDER BY \n",
    "        CASE \n",
    "            WHEN $centrality_field IS NOT NULL THEN n[$centrality_field]\n",
    "            ELSE freq \n",
    "        END DESC,\n",
    "        freq DESC, \n",
    "        weight DESC\n",
    "    LIMIT $topChunks\n",
    "    RETURN e.name AS entityText\n",
    "} AS entity_mapping,\n",
    "// Chunk-Community Mapping\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n:Chunk)-[:ASSOCIATED_WITH]->(c:Community)\n",
    "    WITH c, c.community_rank as rank\n",
    "    RETURN 'Community ' + c.id + ' (rank: ' + toString(rank) + ')' as summary\n",
    "    ORDER BY rank DESC\n",
    "    LIMIT $topCommunities\n",
    "} AS community_mapping,\n",
    "// Related Chunks\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n:Chunk)-[:HAS_ENTITY]->(e:Entity)<-[:HAS_ENTITY]-(m:Chunk)\n",
    "    WHERE NOT m IN nodes\n",
    "    RETURN 'Related chunk shares entity: ' + e.name as descriptionText\n",
    "    LIMIT $topOutsideRels\n",
    "} as related_chunks,\n",
    "// Community Chunks\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    MATCH (n:Chunk)-[:ASSOCIATED_WITH]->(c:Community)<-[:ASSOCIATED_WITH]-(m:Chunk)\n",
    "    WHERE m IN nodes AND id(n) < id(m)\n",
    "    RETURN 'Chunks in community: ' + c.id as descriptionText\n",
    "    LIMIT $topInsideRels\n",
    "} as community_chunks,\n",
    "// Chunk content\n",
    "collect {\n",
    "    UNWIND nodes as n\n",
    "    RETURN n.text as descriptionText\n",
    "} as chunks\n",
    "RETURN {\n",
    "    Entities: entity_mapping,\n",
    "    Communities: community_mapping,\n",
    "    RelatedChunks: related_chunks,\n",
    "    CommunityChunks: community_chunks,\n",
    "    Content: chunks\n",
    "} AS text, 1.0 AS score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "class LocalHuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "# Use the wrapper class\n",
    "embeddings = LocalHuggingFaceEmbeddings('all-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_community_weights(graph):\n",
    "    result = graph.query(\"\"\"\n",
    "    MATCH (n:Community)<-[:BELONGS_TO]-(e:Entity)<-[:HAS_ENTITY]-(c:Chunk)\n",
    "    WITH n, count(distinct c) AS chunkCount\n",
    "    SET n.weight = chunkCount,\n",
    "        // Add community rank if not exists\n",
    "        n.community_rank = coalesce(n.community_rank, 0)\n",
    "    RETURN count(n) as communities_weighted\n",
    "    \"\"\")\n",
    "    print(f\"Community weights calculated for {result} communities\")\n",
    "\n",
    "def create_embeddings_and_store():\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        # Create vector index for Event nodes with configurable dimensions\n",
    "        session.run(\"\"\"\n",
    "        CREATE VECTOR INDEX event_vector IF NOT EXISTS\n",
    "        FOR (e:Event)\n",
    "        ON e.embedding\n",
    "        OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 384,  // Matches the all-MiniLM-L6-v2 model dimensions\n",
    "            `vector.similarity_function`: 'cosine'\n",
    "        }}\n",
    "        \"\"\")\n",
    "\n",
    "        # Create vector index for Chunk nodes\n",
    "        session.run(\"\"\"\n",
    "        CREATE VECTOR INDEX chunk_vector IF NOT EXISTS\n",
    "        FOR (c:Chunk)\n",
    "        ON c.embedding\n",
    "        OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 384,  // Matches the all-MiniLM-L6-v2 model dimensions\n",
    "            `vector.similarity_function`: 'cosine'\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Fetch all Event nodes with dialogue\n",
    "        event_result = session.run(\"\"\"\n",
    "            MATCH (e:Event)\n",
    "            WHERE e.dialogue IS NOT NULL\n",
    "            RETURN e.id AS id, e.dialogue AS dialogue\n",
    "        \"\"\")\n",
    "        \n",
    "        events = [(record[\"id\"], record[\"dialogue\"]) for record in event_result]\n",
    "\n",
    "        # Fetch all Chunk nodes with text\n",
    "        chunk_result = session.run(\"\"\"\n",
    "            MATCH (c:Chunk)\n",
    "            WHERE c.text IS NOT NULL\n",
    "            RETURN c.id AS id, c.text AS text\n",
    "        \"\"\")\n",
    "        \n",
    "        chunks = [(record[\"id\"], record[\"text\"]) for record in chunk_result]\n",
    "\n",
    "        # Create embeddings for Events\n",
    "        batch_size = 20\n",
    "        for i in range(0, len(events), batch_size):\n",
    "            batch = events[i:i+batch_size]\n",
    "            texts = [dialogue for _, dialogue in batch]\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings_batch = embeddings.embed_documents(texts)\n",
    "            \n",
    "            # Store embeddings in Neo4j\n",
    "            for (node_id, _), embedding in zip(batch, embeddings_batch):\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (e:Event {id: $node_id})\n",
    "                    CALL db.create.setNodeVectorProperty(e, 'embedding', $embedding)\n",
    "                    RETURN e\n",
    "                \"\"\", node_id=node_id, embedding=embedding)\n",
    "\n",
    "        # Create embeddings for Chunks\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i+batch_size]\n",
    "            texts = [text for _, text in batch]\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings_batch = embeddings.embed_documents(texts)\n",
    "            \n",
    "            # Store embeddings in Neo4j\n",
    "            for (node_id, _), embedding in zip(batch, embeddings_batch):\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (c:Chunk {id: $node_id})\n",
    "                    CALL db.create.setNodeVectorProperty(c, 'embedding', $embedding)\n",
    "                    RETURN c\n",
    "                \"\"\", node_id=node_id, embedding=embedding)\n",
    "        \n",
    "        print(f\"Embeddings created and stored for {len(events)} Event nodes and {len(chunks)} Chunk nodes.\")\n",
    "\n",
    "def create_neo4j_vectors():\n",
    "    # Create Neo4jVector instances\n",
    "    event_vector = Neo4jVector.from_existing_index(\n",
    "        embedding=embeddings,\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_USERNAME,\n",
    "        password=NEO4J_PASSWORD,\n",
    "        index_name=\"event_vector\",\n",
    "        node_label=\"Event\",\n",
    "        text_node_property=\"dialogue\",\n",
    "        embedding_node_property=\"embedding\",\n",
    "        retrieval_query = ev_retrieval_query\n",
    "    )\n",
    "\n",
    "    chunk_vector = Neo4jVector.from_existing_index(\n",
    "        embedding=embeddings,\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_USERNAME,\n",
    "        password=NEO4J_PASSWORD,\n",
    "        index_name=\"chunk_vector\",\n",
    "        node_label=\"Chunk\",\n",
    "        text_node_property=\"text\",\n",
    "        embedding_node_property=\"embedding\",\n",
    "        retrieval_query = cv_retrieval_query\n",
    "    )\n",
    "\n",
    "    return event_vector, chunk_vector\n",
    "\n",
    "def similarity_search(query, vector_store, k=3, order_by_centrality=None):\n",
    "    \"\"\"\n",
    "    Perform similarity search with optional centrality-based ordering\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        vector_store: Neo4jVector instance\n",
    "        k: Number of results to return\n",
    "        order_by_centrality: One of ['degree', 'betweenness', 'eigenvector'] or None\n",
    "    \"\"\"\n",
    "    # Base parameters that are always needed\n",
    "    params = {\n",
    "        \"topChunks\": topChunks,\n",
    "        \"topCommunities\": topCommunities,\n",
    "        \"topOutsideRels\": topOutsideRels,\n",
    "        \"topInsideRels\": topInsideRels,\n",
    "        \"topEntities\": topEntities,\n",
    "        \"centrality_field\": None  # Default to None when not using centrality\n",
    "    }\n",
    "    \n",
    "    # Modify centrality field if specified\n",
    "    if order_by_centrality:\n",
    "        centrality_field = {\n",
    "            'degree': 'degree_centrality',\n",
    "            'betweenness': 'betweenness_centrality',\n",
    "            'eigenvector': 'eigenvector_centrality'\n",
    "        }.get(order_by_centrality)\n",
    "        \n",
    "        if not centrality_field:\n",
    "            raise ValueError(\"order_by_centrality must be one of: degree, betweenness, eigenvector\")\n",
    "        \n",
    "        params[\"centrality_field\"] = centrality_field\n",
    "\n",
    "    results = vector_store.similarity_search(query, k=k, params=params)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create embeddings and store them\n",
    "    create_embeddings_and_store()\n",
    "    \n",
    "    # Create Neo4jVector instances\n",
    "    event_vector, chunk_vector = create_neo4j_vectors()\n",
    "    \n",
    "    # Example similarity search for Event nodes\n",
    "    event_query = \"A character's dialogue about a mysterious event\"\n",
    "    event_results = similarity_search(event_query, event_vector)\n",
    "\n",
    "    # Order by degree centrality\n",
    "    results_degree = similarity_search(event_query, event_vector, k=3, order_by_centrality='degree')\n",
    "\n",
    "    # Order by betweenness centrality\n",
    "    results_betweenness = similarity_search(event_query, event_vector, k=3, order_by_centrality='betweenness')\n",
    "\n",
    "    # Order by eigenvector centrality\n",
    "    results_eigenvector = similarity_search(event_query, event_vector, k=3, order_by_centrality='eigenvector')\n",
    "\n",
    "    # Default ordering (no centrality)\n",
    "    results_default = similarity_search(event_query, event_vector, k=3)\n",
    "    \n",
    "    print(f\"\\nTop 3 similar Event nodes for query: '{event_query}'\")\n",
    "    for i, result in enumerate(event_results, 1):\n",
    "        print(f\"{i}. {result.page_content}\")\n",
    "\n",
    "    # Example similarity search for Chunk nodes\n",
    "    chunk_query = \"A character's dialogue about a mysterious event\"\n",
    "    chunk_results = similarity_search(chunk_query, chunk_vector)\n",
    "    \n",
    "    print(f\"\\nTop 3 similar Chunk nodes for query: '{chunk_query}'\")\n",
    "    for i, result in enumerate(chunk_results, 1):\n",
    "        print(f\"{i}. {result.page_content}\")\n",
    "\n",
    "driver.close()\n",
    "\n",
    "### Retry if you can't connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 similar Event nodes for query: 'A character's dialogue about a mysterious event'\n",
      "1. Communities:\n",
      "DialogueRelationships:\n",
      "Events:\n",
      "- You speak in riddles; the interpretation?\n",
      "- Sits now in restless mood, Knows neither what she would, nor what she should; Broods oer the trinkets night and day, And on him who sent them, more.\n",
      "- Who knows where the four winds have hurried it! A lady took him under her protection At Naples, as he wandered to and fro; She left him many a mark of her affection, As to his lifes end he had cause to know.\n",
      "Chunks:\n",
      "SharedEntities:\n",
      "\n",
      "\n",
      "Top 3 similar Chunk nodes for query: 'A character's dialogue about a mysterious event'\n",
      "1. Communities:\n",
      "DialogueRelationships:\n",
      "Events:\n",
      "- You speak in riddles; the interpretation?\n",
      "- Sits now in restless mood, Knows neither what she would, nor what she should; Broods oer the trinkets night and day, And on him who sent them, more.\n",
      "- Who knows where the four winds have hurried it! A lady took him under her protection At Naples, as he wandered to and fro; She left him many a mark of her affection, As to his lifes end he had cause to know.\n",
      "Chunks:\n",
      "SharedEntities:\n",
      "\n",
      "\n",
      "Top 3 similar Chunk nodes for query: 'A character's dialogue about a mysterious event'\n",
      "1. Communities:\n",
      "DialogueRelationships:\n",
      "Events:\n",
      "- You speak in riddles; the interpretation?\n",
      "- Sits now in restless mood, Knows neither what she would, nor what she should; Broods oer the trinkets night and day, And on him who sent them, more.\n",
      "- Who knows where the four winds have hurried it! A lady took him under her protection At Naples, as he wandered to and fro; She left him many a mark of her affection, As to his lifes end he had cause to know.\n",
      "Chunks:\n",
      "SharedEntities:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTop 3 similar Event nodes for query: '{event_query}'\")\n",
    "for i, result in enumerate(results_degree, 1):\n",
    "    print(f\"{i}. {result.page_content}\")\n",
    "\n",
    "print(f\"\\nTop 3 similar Chunk nodes for query: '{chunk_query}'\")\n",
    "for i, result in enumerate(results_betweenness, 1):\n",
    "    print(f\"{i}. {result.page_content}\")\n",
    "\n",
    "print(f\"\\nTop 3 similar Chunk nodes for query: '{chunk_query}'\")\n",
    "for i, result in enumerate(results_eigenvector, 1):\n",
    "    print(f\"{i}. {result.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LANGGRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial state...\n",
      "Invoking graph...\n",
      "---AGENT PROCESSING---\n",
      "---CHECK RELEVANCE---\n",
      "---QUERY TRANSFORMATION---\n",
      "---AGENT PROCESSING---\n",
      "Graph execution completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.tools import Tool\n",
    "from typing import Annotated, Sequence, Literal, List, Tuple, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "# Create retriever tool using your Neo4j vectors\n",
    "def retrieve_from_neo4j(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant information from Neo4j knowledge graph.\"\"\"\n",
    "    event_results = event_vector.similarity_search(query, k=3)\n",
    "    chunk_results = chunk_vector.similarity_search(query, k=3)\n",
    "    \n",
    "    combined_results = []\n",
    "    for doc in event_results + chunk_results:\n",
    "        if isinstance(doc.page_content, dict):\n",
    "            # Format the structured content\n",
    "            formatted_content = \"\\n\".join([f\"{k}: {v}\" for k, v in doc.page_content.items()])\n",
    "            combined_results.append(formatted_content)\n",
    "        else:\n",
    "            combined_results.append(doc.page_content)\n",
    "    \n",
    "    return \"\\n\\n\".join(combined_results)\n",
    "\n",
    "# Define the retriever tool\n",
    "retriever_tool = Tool(\n",
    "    name=\"retrieve_from_neo4j\",\n",
    "    description=\"Retrieve relevant information from the knowledge graph\",\n",
    "    func=retrieve_from_neo4j\n",
    ")\n",
    "\n",
    "# Define available tools\n",
    "tools = [retriever_tool]\n",
    "\n",
    "def tools_condition(state) -> Literal[\"tools\", \"end\"]:\n",
    "    \"\"\"Determine whether to use tools or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the last message has tool calls, continue with tools\n",
    "    if hasattr(last_message, 'additional_kwargs') and last_message.additional_kwargs.get('tool_calls'):\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "# Define the graph state with proper message handling\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages] \n",
    "    chat_history: List[Tuple[str, str]]\n",
    "    metadata: Dict[str, Any]\n",
    "    retrieval_attempts: int = 0\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"Grade document relevance and decide next action.\"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    \n",
    "    class Grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "    \n",
    "    # Grade using structured output\n",
    "    llm_with_tool = llm.with_structured_output(Grade)\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are grading document relevance to a question.\n",
    "        Document: {context}\n",
    "        Question: {question}\n",
    "        Grade as 'yes' if relevant, 'no' if not.\n",
    "    \"\"\")\n",
    "    \n",
    "    chain = prompt | llm_with_tool\n",
    "    result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "    \n",
    "    return \"generate\" if result.binary_score == \"yes\" else \"rewrite\"\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"Core agent that processes messages and decides actions.\"\"\"\n",
    "    print(\"---AGENT PROCESSING---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = llm.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def rewrite_query(state):\n",
    "    \"\"\"Transform query for better retrieval.\"\"\"\n",
    "    print(\"---QUERY TRANSFORMATION---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    chat_history = state.get(\"chat_history\", [])  # Get chat history with empty default\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Improve this question for better retrieval while:\n",
    "        1. Maintaining original semantic meaning\n",
    "        2. Expanding ambiguous terms\n",
    "        3. Considering chat history context: {chat_history}\n",
    "        \n",
    "        Question: {question}\n",
    "        Improved version:\n",
    "    \"\"\")\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def generate_response(state):\n",
    "    \"\"\"Generate final response from relevant documents.\"\"\"\n",
    "    print(\"---RESPONSE GENERATION---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    context = messages[-1].content\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Answer based on this context:\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "    \"\"\")\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"question\": question, \"context\": context})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Fix the tools_condition function to use END constant\n",
    "def tools_condition(state) -> Literal[\"tools\", \"end\"]:\n",
    "    \"\"\"Determine whether to use tools or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the last message has tool calls, continue with tools\n",
    "    if hasattr(last_message, 'additional_kwargs') and last_message.additional_kwargs.get('tool_calls'):\n",
    "        return \"tools\"\n",
    "    return END \n",
    "\n",
    "def create_agentic_rag():\n",
    "    \"\"\"Create and configure the agentic RAG workflow.\"\"\"\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes with verbose logging\n",
    "    workflow.add_node(\"agent\", agent)\n",
    "    workflow.add_node(\"retrieve\", ToolNode(\n",
    "        tools=[retriever_tool]\n",
    "    ))\n",
    "    workflow.add_node(\"rewrite\", rewrite_query)\n",
    "    workflow.add_node(\"generate\", generate_response)\n",
    "    \n",
    "    # Configure edges\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        tools_condition,\n",
    "        {\n",
    "            \"tools\": \"retrieve\",\n",
    "            END: END  # Use END constant consistently\n",
    "        }\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"retrieve\",\n",
    "        grade_documents,\n",
    "        {\n",
    "            \"generate\": \"generate\",\n",
    "            \"rewrite\": \"rewrite\"\n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    workflow.add_edge(\"rewrite\", \"agent\")\n",
    "    \n",
    "    app = workflow.compile(checkpointer=MemorySaver())\n",
    "    app.step_timeout = 30  # Increase timeout to avoid potential race conditions\n",
    "    return app\n",
    "\n",
    "def query_agentic_rag(question: str):\n",
    "    \"\"\"Execute agentic RAG query.\"\"\"\n",
    "    graph = create_agentic_rag()\n",
    "    \n",
    "    print(\"Creating initial state...\")\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=question)],\n",
    "        \"chat_history\": [],\n",
    "        \"metadata\": {\"debug\": True},\n",
    "        \"retrieval_attempts\": 0\n",
    "    }\n",
    "    \n",
    "    print(\"Invoking graph...\")\n",
    "    try:\n",
    "        # Use RunnableConfig for proper configuration\n",
    "        from langchain_core.runnables import RunnableConfig\n",
    "        \n",
    "        config = RunnableConfig(\n",
    "            recursion_limit=25,  # Set recursion limit if needed\n",
    "            configurable={\n",
    "                \"thread_id\": \"rag_session\",\n",
    "                \"metadata\": {\"debug\": True}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        result = graph.invoke(initial_state, config)\n",
    "        print(\"Graph execution completed.\")\n",
    "        return result[\"messages\"][-1].content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during graph execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "question = \"What happens in the first scene of Faust?\"\n",
    "response = query_agentic_rag(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
